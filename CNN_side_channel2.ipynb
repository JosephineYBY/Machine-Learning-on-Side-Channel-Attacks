{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import scipy.io as sio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x=sio.loadmat('E:\\\\Biyun Yan\\\\wpi\\machine learning in cyber scurity\\\\pro5\\\\data\\\\Training_data_40classes_Tor_Browser.mat')\n",
    "train_data=train_x['X_train3']\n",
    "train_y=sio.loadmat('E:\\\\Biyun Yan\\\\wpi\\\\machine learning in cyber scurity\\\\pro5\\\\data\\\\Training_label_40classes_Tor_Browser.mat')\n",
    "train_label=(train_y['Ytrain']-1).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x=sio.loadmat('E:\\\\Biyun Yan\\\\wpi\\\\machine learning in cyber scurity\\\\pro5\\\\data\\\\Test_data_40classes_Tor_Browser.mat')\n",
    "test_data=test_x['X_test']\n",
    "test_y=sio.loadmat('E:\\\\Biyun Yan\\\\wpi\\\\machine learning in cyber scurity\\\\pro5\\\\data\\\\Test_label_40classes_Tor_Browser.mat')\n",
    "test_label=(test_y['Ytest']-1).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-4-df0f6d518da3>:3: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.conv2d instead.\n",
      "WARNING:tensorflow:From C:\\Users\\motto\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From <ipython-input-4-df0f6d518da3>:4: max_pooling2d (from tensorflow.python.layers.pooling) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.max_pooling2d instead.\n",
      "WARNING:tensorflow:From <ipython-input-4-df0f6d518da3>:10: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.flatten instead.\n",
      "Tensor(\"flatten/Reshape:0\", shape=(?, 750), dtype=float32)\n",
      "WARNING:tensorflow:From <ipython-input-4-df0f6d518da3>:13: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dense instead.\n",
      "WARNING:tensorflow:From <ipython-input-4-df0f6d518da3>:14: arg_max (from tensorflow.python.ops.gen_math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.math.argmax` instead\n",
      "WARNING:tensorflow:From <ipython-input-4-df0f6d518da3>:15: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x_placeholder=tf.placeholder(tf.float32,[None,100,60,1])\n",
    "y_placeholder=tf.placeholder(tf.int32,[None,1])\n",
    "conv0=tf.layers.conv2d(x_placeholder,5,3,activation=tf.nn.relu,padding='same')\n",
    "pool0=tf.layers.max_pooling2d(conv0,[2,2],[2,2])\n",
    "conv1=tf.layers.conv2d(pool0,2,2,activation=tf.nn.relu,padding='same')\n",
    "pool1=tf.layers.max_pooling2d(conv1,[2,2],[2,2])\n",
    "#conv2=tf.layers.conv2d(pool0,60,2,activation=tf.nn.relu,padding='same')\n",
    "#pool2=tf.layers.max_pooling2d(conv2,[2,2],[2,2])\n",
    "\n",
    "flatten=tf.layers.flatten(pool1)\n",
    "print(flatten)\n",
    "#fc=tf.layers.dense(flatten,20,activation=tf.nn.relu)\n",
    "logits=tf.layers.dense(flatten,40)\n",
    "predicted_labels=tf.arg_max(logits,1)\n",
    "losses=tf.nn.softmax_cross_entropy_with_logits(labels=tf.one_hot(y_placeholder,40),logits=logits)\n",
    "mean_loss=tf.reduce_mean(losses)\n",
    "optimizer=tf.train.AdamOptimizer(learning_rate=0.01).minimize(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x=np.reshape(train_data,[-1,100,60,1])\n",
    "train_y=train_label\n",
    "test_x=np.reshape(test_data,[-1,100,60,1])\n",
    "test_y=test_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 3.7635033 0.0 0.0\n",
      "1 3.7146542 55.0 0.034375\n",
      "saving model\n",
      "2 3.7145422 61.0 0.038125\n",
      "saving model\n",
      "3 3.7245996 85.0 0.053125\n",
      "saving model\n",
      "4 3.799542 69.0 0.043125\n",
      "5 3.7838535 47.0 0.029375\n",
      "6 3.6626732 47.0 0.029375\n",
      "7 3.588197 68.0 0.0425\n",
      "8 3.5482845 71.0 0.044375\n",
      "9 3.4962504 68.0 0.0425\n",
      "10 3.448375 80.0 0.05\n",
      "11 3.421399 106.0 0.06625\n",
      "saving model\n",
      "12 3.4084945 82.0 0.05125\n",
      "13 3.3953304 66.0 0.04125\n",
      "14 3.382385 127.0 0.079375\n",
      "saving model\n",
      "15 3.369702 165.0 0.103125\n",
      "saving model\n",
      "16 3.347299 166.0 0.10375\n",
      "saving model\n",
      "17 3.3178988 208.0 0.13\n",
      "saving model\n",
      "18 3.290072 234.0 0.14625\n",
      "saving model\n",
      "19 3.2631586 246.0 0.15375\n",
      "saving model\n",
      "20 3.2380152 256.0 0.16\n",
      "saving model\n",
      "21 3.2170038 254.0 0.15875\n",
      "22 3.1970289 260.0 0.1625\n",
      "saving model\n",
      "23 3.175522 269.0 0.168125\n",
      "saving model\n",
      "24 3.152188 271.0 0.169375\n",
      "saving model\n",
      "25 3.142724 294.0 0.18375\n",
      "saving model\n",
      "26 3.1228344 287.0 0.179375\n",
      "27 3.0995057 281.0 0.175625\n",
      "28 3.0907671 298.0 0.18625\n",
      "saving model\n",
      "29 3.0856733 317.0 0.198125\n",
      "saving model\n",
      "30 3.0797143 333.0 0.208125\n",
      "saving model\n",
      "31 3.0617101 297.0 0.185625\n",
      "32 3.0691452 335.0 0.209375\n",
      "saving model\n",
      "33 3.0691137 329.0 0.205625\n",
      "34 3.0279665 347.0 0.216875\n",
      "saving model\n",
      "35 3.0048418 337.0 0.210625\n",
      "36 3.024477 351.0 0.219375\n",
      "saving model\n",
      "37 2.9956195 356.0 0.2225\n",
      "saving model\n",
      "38 2.9718335 346.0 0.21625\n",
      "39 2.9782836 363.0 0.226875\n",
      "saving model\n",
      "40 2.9670813 366.0 0.22875\n",
      "saving model\n",
      "41 2.9443316 352.0 0.22\n",
      "42 2.9447477 361.0 0.225625\n",
      "43 2.934999 381.0 0.238125\n",
      "saving model\n",
      "44 2.9068995 359.0 0.224375\n",
      "45 2.8999548 358.0 0.22375\n",
      "46 2.8715546 314.0 0.19625\n",
      "47 2.830384 371.0 0.231875\n",
      "48 2.810281 354.0 0.22125\n",
      "49 2.7983541 370.0 0.23125\n",
      "50 2.7620878 362.0 0.22625\n",
      "51 2.7571323 380.0 0.2375\n",
      "52 2.7383447 365.0 0.228125\n",
      "53 2.725879 361.0 0.225625\n",
      "54 2.7289202 342.0 0.21375\n",
      "55 2.7279797 363.0 0.226875\n",
      "56 2.707541 353.0 0.220625\n",
      "57 2.7128372 372.0 0.2325\n",
      "58 2.7039213 357.0 0.223125\n",
      "59 2.703673 374.0 0.23375\n",
      "60 2.6907403 367.0 0.229375\n",
      "61 2.6865597 388.0 0.2425\n",
      "saving model\n",
      "62 2.6766977 370.0 0.23125\n",
      "63 2.6721795 394.0 0.24625\n",
      "saving model\n",
      "64 2.658061 374.0 0.23375\n",
      "65 2.6548872 394.0 0.24625\n",
      "66 2.6385088 384.0 0.24\n",
      "67 2.6339164 434.0 0.27125\n",
      "saving model\n",
      "68 2.6213822 414.0 0.25875\n",
      "69 2.614055 436.0 0.2725\n",
      "saving model\n",
      "70 2.601475 407.0 0.254375\n",
      "71 2.599212 424.0 0.265\n",
      "72 2.5857239 417.0 0.260625\n",
      "73 2.5870066 435.0 0.271875\n",
      "74 2.574734 423.0 0.264375\n",
      "75 2.5770655 438.0 0.27375\n",
      "saving model\n",
      "76 2.5631957 431.0 0.269375\n",
      "77 2.5650067 444.0 0.2775\n",
      "saving model\n",
      "78 2.5510602 439.0 0.274375\n",
      "79 2.5556328 457.0 0.285625\n",
      "saving model\n",
      "80 2.5408726 440.0 0.275\n",
      "81 2.5468478 455.0 0.284375\n",
      "82 2.5324948 449.0 0.280625\n",
      "83 2.5396862 469.0 0.293125\n",
      "saving model\n",
      "84 2.524861 452.0 0.2825\n",
      "85 2.5326657 472.0 0.295\n",
      "saving model\n",
      "86 2.5159738 455.0 0.284375\n",
      "87 2.5295494 467.0 0.291875\n",
      "88 2.50627 477.0 0.298125\n",
      "saving model\n",
      "89 2.532024 489.0 0.305625\n",
      "saving model\n",
      "90 2.4971395 484.0 0.3025\n",
      "91 2.53111 490.0 0.30625\n",
      "saving model\n",
      "92 2.4952087 487.0 0.304375\n",
      "93 2.5283353 490.0 0.30625\n",
      "94 2.4874358 487.0 0.304375\n",
      "95 2.500275 498.0 0.31125\n",
      "saving model\n",
      "96 2.4952438 491.0 0.306875\n",
      "97 2.475151 514.0 0.32125\n",
      "saving model\n",
      "98 2.491651 479.0 0.299375\n",
      "99 2.4627848 510.0 0.31875\n",
      "100 2.4792445 497.0 0.310625\n",
      "101 2.4631934 521.0 0.325625\n",
      "saving model\n",
      "102 2.465757 528.0 0.33\n",
      "saving model\n",
      "103 2.4660075 539.0 0.336875\n",
      "saving model\n",
      "104 2.4615202 528.0 0.33\n",
      "105 2.4513865 532.0 0.3325\n",
      "106 2.463406 498.0 0.31125\n",
      "107 2.447498 533.0 0.333125\n",
      "108 2.4438138 537.0 0.335625\n",
      "109 2.4479132 551.0 0.344375\n",
      "saving model\n",
      "110 2.4383667 529.0 0.330625\n",
      "111 2.4323316 560.0 0.35\n",
      "saving model\n",
      "112 2.415374 537.0 0.335625\n",
      "113 2.4336388 550.0 0.34375\n",
      "114 2.415419 545.0 0.340625\n",
      "115 2.4216247 560.0 0.35\n",
      "116 2.414712 550.0 0.34375\n",
      "117 2.4091387 563.0 0.351875\n",
      "saving model\n",
      "118 2.4292428 536.0 0.335\n",
      "119 2.4323726 555.0 0.346875\n",
      "120 2.4082804 560.0 0.35\n",
      "121 2.4295633 563.0 0.351875\n",
      "122 2.412863 551.0 0.344375\n",
      "123 2.4068182 565.0 0.353125\n",
      "saving model\n",
      "124 2.376624 553.0 0.345625\n",
      "125 2.3783329 590.0 0.36875\n",
      "saving model\n",
      "126 2.3842094 565.0 0.353125\n",
      "127 2.3804328 588.0 0.3675\n",
      "128 2.3675272 571.0 0.356875\n",
      "129 2.373405 590.0 0.36875\n",
      "130 2.375163 578.0 0.36125\n",
      "131 2.3729572 593.0 0.370625\n",
      "saving model\n",
      "132 2.357936 580.0 0.3625\n",
      "133 2.3606968 595.0 0.371875\n",
      "saving model\n",
      "134 2.355613 581.0 0.363125\n",
      "135 2.3593318 597.0 0.373125\n",
      "saving model\n",
      "136 2.3471277 588.0 0.3675\n",
      "137 2.351044 603.0 0.376875\n",
      "saving model\n",
      "138 2.3418016 590.0 0.36875\n",
      "139 2.3465354 605.0 0.378125\n",
      "saving model\n",
      "140 2.3355448 596.0 0.3725\n",
      "141 2.3404033 610.0 0.38125\n",
      "saving model\n",
      "142 2.330428 603.0 0.376875\n",
      "143 2.3341384 616.0 0.385\n",
      "saving model\n",
      "144 2.3252616 601.0 0.375625\n",
      "145 2.3292978 617.0 0.385625\n",
      "saving model\n",
      "146 2.321475 604.0 0.3775\n",
      "147 2.3242528 623.0 0.389375\n",
      "saving model\n",
      "148 2.3173108 610.0 0.38125\n",
      "149 2.3202124 625.0 0.390625\n",
      "saving model\n",
      "150 2.3109086 617.0 0.385625\n",
      "151 2.3172052 625.0 0.390625\n",
      "152 2.306248 616.0 0.385\n",
      "153 2.3140497 628.0 0.3925\n",
      "saving model\n",
      "154 2.3014994 618.0 0.38625\n",
      "155 2.310544 629.0 0.393125\n",
      "saving model\n",
      "156 2.2985194 620.0 0.3875\n",
      "157 2.3064978 636.0 0.3975\n",
      "saving model\n",
      "158 2.2953434 625.0 0.390625\n",
      "159 2.3017316 643.0 0.401875\n",
      "saving model\n",
      "160 2.2914484 634.0 0.39625\n",
      "161 2.2983422 649.0 0.405625\n",
      "saving model\n",
      "162 2.287181 635.0 0.396875\n",
      "163 2.296107 650.0 0.40625\n",
      "saving model\n",
      "164 2.282658 639.0 0.399375\n",
      "165 2.294732 654.0 0.40875\n",
      "saving model\n",
      "166 2.279324 643.0 0.401875\n",
      "167 2.2902248 656.0 0.41\n",
      "saving model\n",
      "168 2.2759979 647.0 0.404375\n",
      "169 2.2844164 663.0 0.414375\n",
      "saving model\n",
      "170 2.2733161 652.0 0.4075\n",
      "171 2.281141 665.0 0.415625\n",
      "saving model\n",
      "172 2.2711358 654.0 0.40875\n",
      "173 2.2762742 670.0 0.41875\n",
      "saving model\n",
      "174 2.268885 659.0 0.411875\n",
      "175 2.2726808 676.0 0.4225\n",
      "saving model\n",
      "176 2.2653785 661.0 0.413125\n",
      "177 2.2692652 679.0 0.424375\n",
      "saving model\n",
      "178 2.2614307 669.0 0.418125\n",
      "179 2.2656589 684.0 0.4275\n",
      "saving model\n",
      "180 2.2571888 668.0 0.4175\n",
      "181 2.2617347 687.0 0.429375\n",
      "saving model\n",
      "182 2.2509682 671.0 0.419375\n",
      "183 2.252833 691.0 0.431875\n",
      "saving model\n",
      "184 2.2411761 677.0 0.423125\n",
      "185 2.237958 692.0 0.4325\n",
      "saving model\n",
      "186 2.2250075 675.0 0.421875\n",
      "187 2.2196548 691.0 0.431875\n",
      "188 2.2073765 679.0 0.424375\n",
      "189 2.1991844 696.0 0.435\n",
      "saving model\n",
      "190 2.1905875 690.0 0.43125\n",
      "191 2.1839857 698.0 0.43625\n",
      "saving model\n",
      "192 2.1803403 692.0 0.4325\n",
      "193 2.1723063 705.0 0.440625\n",
      "saving model\n",
      "194 2.1711729 691.0 0.431875\n",
      "195 2.1578286 711.0 0.444375\n",
      "saving model\n",
      "196 2.1556067 703.0 0.439375\n",
      "197 2.1386595 718.0 0.44875\n",
      "saving model\n",
      "198 2.1427135 716.0 0.4475\n",
      "199 2.124339 728.0 0.455\n",
      "saving model\n",
      "200 2.1291687 749.0 0.468125\n",
      "saving model\n",
      "201 2.1081064 753.0 0.470625\n",
      "saving model\n",
      "202 2.114511 754.0 0.47125\n",
      "saving model\n",
      "203 2.0972526 745.0 0.465625\n",
      "204 2.0952997 755.0 0.471875\n",
      "saving model\n",
      "205 2.0990024 761.0 0.475625\n",
      "saving model\n",
      "206 2.0643978 762.0 0.47625\n",
      "saving model\n",
      "207 2.0878813 769.0 0.480625\n",
      "saving model\n",
      "208 2.051368 784.0 0.49\n",
      "saving model\n",
      "209 2.067699 798.0 0.49875\n",
      "saving model\n",
      "210 2.0447059 795.0 0.496875\n",
      "211 2.0480533 793.0 0.495625\n",
      "212 2.0380747 802.0 0.50125\n",
      "saving model\n",
      "213 2.0192783 796.0 0.4975\n",
      "214 2.0295408 815.0 0.509375\n",
      "saving model\n",
      "215 1.9986895 827.0 0.516875\n",
      "saving model\n",
      "216 2.005823 838.0 0.52375\n",
      "saving model\n",
      "217 1.9946191 852.0 0.5325\n",
      "saving model\n",
      "218 1.9890305 862.0 0.53875\n",
      "saving model\n",
      "219 1.9748101 854.0 0.53375\n",
      "220 2.0149398 857.0 0.535625\n",
      "221 1.9877827 802.0 0.50125\n",
      "222 1.9843092 845.0 0.528125\n",
      "223 1.948359 833.0 0.520625\n",
      "224 1.9568597 847.0 0.529375\n",
      "225 1.942259 867.0 0.541875\n",
      "saving model\n",
      "226 1.9376782 885.0 0.553125\n",
      "saving model\n",
      "227 1.9311038 852.0 0.5325\n",
      "228 1.8731368 876.0 0.5475\n",
      "229 2.176755 723.0 0.451875\n",
      "230 2.0490212 751.0 0.469375\n",
      "231 1.9473546 795.0 0.496875\n",
      "232 1.896817 840.0 0.525\n",
      "233 1.9230523 859.0 0.536875\n",
      "234 1.9127616 868.0 0.5425\n",
      "235 1.8815482 867.0 0.541875\n",
      "236 1.840589 905.0 0.565625\n",
      "saving model\n",
      "237 1.8347669 924.0 0.5775\n",
      "saving model\n",
      "238 1.8411605 906.0 0.56625\n",
      "239 1.8459232 918.0 0.57375\n",
      "240 1.8153872 923.0 0.576875\n",
      "241 1.8125153 918.0 0.57375\n",
      "242 1.8204966 919.0 0.574375\n",
      "243 1.8044713 926.0 0.57875\n",
      "saving model\n",
      "244 1.7878133 929.0 0.580625\n",
      "saving model\n",
      "245 1.7723093 926.0 0.57875\n",
      "246 1.7861768 926.0 0.57875\n",
      "247 1.7802054 940.0 0.5875\n",
      "saving model\n",
      "248 1.7478851 934.0 0.58375\n",
      "249 1.7424109 939.0 0.586875\n",
      "250 1.7557987 938.0 0.58625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "251 1.7557759 951.0 0.594375\n",
      "saving model\n",
      "252 1.7240704 938.0 0.58625\n",
      "253 1.7123963 939.0 0.586875\n",
      "254 1.7245048 950.0 0.59375\n",
      "255 1.734304 915.0 0.571875\n",
      "256 1.7613606 933.0 0.583125\n",
      "257 1.7275238 939.0 0.586875\n",
      "258 1.6938982 941.0 0.588125\n",
      "259 1.7318463 928.0 0.58\n",
      "260 1.6944219 954.0 0.59625\n",
      "saving model\n",
      "261 1.6995325 948.0 0.5925\n",
      "262 1.62274 895.0 0.559375\n",
      "263 1.8436583 859.0 0.536875\n",
      "264 1.7732959 893.0 0.558125\n",
      "265 1.5956177 904.0 0.565\n",
      "266 1.6617675 925.0 0.578125\n",
      "267 1.7068162 916.0 0.5725\n",
      "268 1.6900537 934.0 0.58375\n",
      "269 1.6325253 963.0 0.601875\n",
      "saving model\n",
      "270 1.6262774 967.0 0.604375\n",
      "saving model\n",
      "271 1.6651719 957.0 0.598125\n",
      "272 1.6216402 979.0 0.611875\n",
      "saving model\n",
      "273 1.6093833 976.0 0.61\n",
      "274 1.6979241 970.0 0.60625\n",
      "275 1.5519955 964.0 0.6025\n",
      "276 1.5785478 978.0 0.61125\n",
      "277 1.6404824 976.0 0.61\n",
      "278 1.5602057 966.0 0.60375\n",
      "279 1.5732875 978.0 0.61125\n",
      "280 1.6066699 988.0 0.6175\n",
      "saving model\n",
      "281 1.5130489 999.0 0.624375\n",
      "saving model\n",
      "282 1.489592 1005.0 0.628125\n",
      "saving model\n",
      "283 1.5112094 980.0 0.6125\n",
      "284 1.5431724 994.0 0.62125\n",
      "285 1.6143537 983.0 0.614375\n",
      "286 1.548598 961.0 0.600625\n",
      "287 1.561745 980.0 0.6125\n",
      "288 1.5299478 985.0 0.615625\n",
      "289 1.4915793 972.0 0.6075\n",
      "290 1.4934573 991.0 0.619375\n",
      "291 1.5061591 1010.0 0.63125\n",
      "saving model\n",
      "292 1.4073646 1031.0 0.644375\n",
      "saving model\n",
      "293 1.3799155 1041.0 0.650625\n",
      "saving model\n",
      "294 1.4058381 1035.0 0.646875\n",
      "295 1.3611706 1043.0 0.651875\n",
      "saving model\n",
      "296 1.3324322 1052.0 0.6575\n",
      "saving model\n",
      "297 1.3638895 1049.0 0.655625\n",
      "298 1.3283246 1072.0 0.67\n",
      "saving model\n",
      "299 1.2998408 1069.0 0.668125\n"
     ]
    }
   ],
   "source": [
    "saver=tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    #val_feed_dict={x_placeholder:val_x,y_placeholder:val_y}\n",
    "    \n",
    "    max_acc=0\n",
    "    for step in range(300):\n",
    "        start=0\n",
    "        batch_size=400\n",
    "        correct=0.\n",
    "        for i in range(4):\n",
    "            train_feed_dict={x_placeholder:train_x[start:start+batch_size],y_placeholder:train_y[start:start+batch_size]}\n",
    "            \n",
    "            \n",
    "            _,mean_loss_val,predicted_labels_train_batch=sess.run([optimizer,mean_loss,predicted_labels],feed_dict=train_feed_dict)\n",
    "            correct+=np.sum(predicted_labels_train_batch.reshape(-1,1)==train_y[start:start+batch_size])\n",
    "            #print(predicted_labels_train_batch.reshape(-1,1))\n",
    "            #print(train_y[start:start+batch_size])\n",
    "            #print(\"correct number\",i,correct)\n",
    "            start=start+batch_size\n",
    "            #print(mean_loss_val)\n",
    "        #predicted_labels_validation=sess.run([predicted_labels],feed_dict=val_feed_dict)\n",
    "        acc=correct/train_x.shape[0]\n",
    "        print(step,mean_loss_val,correct,acc)\n",
    "        if max_acc<acc:\n",
    "            max_acc=acc\n",
    "            print(\"saving model\")\n",
    "            saver.save(sess,'model_cnn/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\motto\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n",
      "INFO:tensorflow:Restoring parameters from model_cnn/\n",
      "0.68\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    saver.restore(sess,'model_cnn/')\n",
    "    test_feed_dict={x_placeholder:test_x,y_placeholder:test_y}\n",
    "    predicted_labels_val=sess.run(predicted_labels,feed_dict=test_feed_dict)\n",
    "    acc=np.sum(predicted_labels_val.reshape(-1,1)==test_y)/test_x.shape[0]\n",
    "    print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
